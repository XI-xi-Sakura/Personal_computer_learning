#  哨兵 (Sentinel)



Redis 的主从复制模式下，一旦主节点由于故障不能提供服务，需要人工进行主从切换，同时大量的客户端需要被通知切换到新的主节点上，对于上了一定规模的应用来说，这种方案是无法接受的。


于是 Redis 从 2.8 开始提供了 Redis Sentinel（哨兵）加个来解决这个问题。本章主要内容如下：
- Redis Sentinel 的概念
- Redis Sentinel 的部署
- Redis Sentinel 命令
- Redis Sentinel 客户端
- Redis Sentinel 实现原理

在复制章节中,我们说了给一个老师配了几个助教.如果老师有事请假了几天,助教又不能上课的话,学生们的学习进度就会受到影响.哨兵就是用来解决这个问题的.

## 基本概念
由于对 Redis 的许多概念都有不同的名词解释，所以在介绍 Redis Sentinel 之前，先对几个名词概念进行必要的说明，如表所示。

 **Redis Sentinel相关名词解释**
|名词|逻辑结构|物理结构|
| ---- | ---- | ---- |
|主节点|Redis 主服务|一个独立的 redis-server 进程|
|从节点|Redis 从服务|一个独立的 redis-server 进程|
|Redis 数据节点|主从节点|主节点和从节点的进程|
|哨兵节点|监控 Redis 数据节点的节点|一个独立的 redis-sentinel 进程|
|哨兵节点集合|若干哨兵节点的抽象组合|若干 redis-sentinel 进程|
|Redis 哨兵（Sentinel）|Redis 提供的高可用方案|哨兵节点集合和 Redis 主从节点|
|应用方|泛指一个多多个客户端|一个或多个连接 Redis 的进程|

Redis Sentinel 是 Redis 的高可用实现方案，在实际的生产环境中，对提高整个系统的高可用是非常有帮助的。

## 主从复制的问题
Redis 的主从复制模式可以将主节点的数据改变同步给从节点，这样从节点就可以起到两个作用：

- 第一，作为主节点的一个备份，一旦主节点出了故障不可达的情况，从节点可以作为后备 “顶” 上来，并且保证数据尽量不丢失（主从复制表现为最终一致性）。
- 第二，从节点可以分担主节点上的读压力，让主节点只承担写请求的处理，将所有的读请求负载均衡到各个从节点上。

但是主从复制模式并不是万能的，它同样遗留下以下几个问题：
1. 主节点发生故障时，进行主备切换的过程是复杂的，需要完全的人工参与，导致故障恢复时间无法保障。
2. 主节点可以将读压力分散出去，但写压力/存储压力是无法被分担的，还是受到单机的限制。

其中第一个问题是高可用问题，即 Redis 哨兵主要解决的问题。第二个问题是属于存储分布式的问题，留给 Redis 集群去解决，本章我们集中讨论第一个问题。



## 哨兵自动恢复主节点故障
当主节点出现故障时，Redis Sentinel能自动完成故障发现和故障转移，并通知应用方，从而实现真正的高可用。

Redis Sentinel是一个分布式架构，其中包含若干个Sentinel节点和Redis数据节点，每个Sentinel节点会对数据节点和其余Sentinel节点进行监控，当它发现节点不可达时，会对节点做下线表示。

如果下线的是主节点，它还会和其他的Sentinel节点进行“协商”，当大多数Sentinel节点对主节点不可达这个结论达成共识之后，它们会在内部“选举”出一个领导节点来完成自动故障转移的工作，同时将这个变化实时通知给Redis应用方。整个过程是完全自动的，不需要人工介入。整体的架构如图所示。

！ 这里的分布式架构是指：Redis数据节点、Sentinel节点集合、客户端分布在多个物理节点上，不要与后边章节介绍的Redis Cluster分布式混淆。

## Redis Sentinel架构 
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/6187a53363244fa09c1c4231371709fe.png)

Redis Sentinel相比于主从复制模式是多了若干（建议保持奇数）Sentinel节点用于实现监控数据节点，哨兵节点会定期监控所有节点（包含数据节点和其他哨兵节点）。
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/4461fc90405e4feab63fa15886530e30.png)

针对主节点故障的情况，故障转移流程大致如下：
1. 主节点故障，从节点同步连接中断，主从复制停止。
2. 哨兵节点通过定期监控发现主节点出现故障。**哨兵节点与其他哨兵节点进行协商，达成多数认同主节点故障的共识**。
>这步主要是防止该情况：出故障的不是主节点，而是发现故障的哨兵节点，该情况经常发生于哨兵节点的网络被孤立的场景下。
3. 哨兵节点**之间使用Raft算法选举出一个领导角色**，由该节点负责后续的故障转移工作。
4. 哨兵领导者开始执行故障转移：**从节点中选择一个作为新主节点**；让其他从节点同步新主节点；通知应用层转移到新主节点。



通过上面的介绍，可以看出Redis Sentinel具有以下几个功能：
- 监控: Sentinel节点会定期检测Redis数据节点、其余哨兵节点是否可达。
- 故障转移: 实现从节点晋升（promotion）为主节点并维护后续正确的主从关系。 
- 通知: Sentinel节点会将故障转移的结果通知给应用方。 

## 安装部署（基于 docker）


 
### 编排 redis 主从节点
**编写 `docker-compose.yml`**
创建 `./yml/redis-data`，同时 cd到该目录中。  
注意: docker 中可以通过容器名字,作为 ip 地址,进行相互之间的访问.  

```yaml
version: '3.7'
services:
  master:
    image: 'redis:5.0.9'
    container_name: redis-master
    restart: always
    command: redis-server --appendonly yes
    ports:
      - 6380:6379
  slave1:
    image: 'redis:5.0.9'
    container_name: redis-slave1
    restart: always
    command: redis-server --appendonly yes --slaveof redis-master 6379
    ports:
      - 6381:6379
  slave2:
    image: 'redis:5.0.9'
    container_name: redis-slave2
    restart: always
    command: redis-server --appendonly yes --slaveof redis-master 6379
    ports:
      - 6382:6379
```

也可以直接在 windows 上使用 vscode 编辑好 yml,然后在上传到 linux 上.  


**启动所有容器**  
```bash
docker-compose up -d
```
如果启动后发现前面的配置有误,需要重新操作,使用 `docker-compose down` 即可停止并删除刚才创建好的容器.  


 **查看运行日志**  
```bash
docker-compose logs
```
上述操作必须保证工作目录在 yml 的同级目录中,才能工作.  


**验证**  
- **连接主节点**  
  ```bash
  redis-cli -p 6380
  ```
  执行 `info replication` 输出：  
  ```
  127.0.0.1:6379> info replication
  # Replication
  role:master
  connected_slaves:2
  slave0:ip=172.22.0.3,port=6379,state=online,offset=348,lag=1
  slave1:ip=172.22.0.4,port=6379,state=online,offset=348,lag=1
  master_replid:22196b425ab42ddf222cc5a64d53acffeb3e63
  master_replid2:0000000000000000000000000000000000000000
  master_repl_offset:348
  second_repl_offset:-1
  repl_backlog_active:1
  repl_backlog_size:1048576
  repl_backlog_first_byte_offset:1
  repl_backlog_histlen:348
  ```

- **连接从节点**  
  - 连接 slave1（端口 6381 ）：  
    ```bash
    redis-cli -p 6380
    ```
    执行 `info replication` 输出：  
    ```
    127.0.0.1:6380> info replication
    # Replication
    role:slave
    master_host:redis-master
    master_port:6379
    master_link_status:up
    master_last_io_seconds_ago:10
    master_sync_in_progress:0
    slave_repl_offset:446
    slave_priority:100
    slave_read_only:1
    connected_slaves:0
    master_replid:22196b425ab42ddf222cc5a64d53acffeb3e63
    master_replid2:0000000000000000000000000000000000000000
    master_repl_offset:446
    second_repl_offset:-1
    repl_backlog_active:1
    repl_backlog_size:1048576
    repl_backlog_first_byte_offset:1
    repl_backlog_histlen:446
    ```

  - 连接 slave2（端口 6382 ）：  
    ```bash
    redis-cli -p 6381
    ```
    执行 `info replication` 输出：  
    ```
    127.0.0.1:6381> info replication
    # Replication
    role:slave
    master_host:redis-master
    master_port:6379
    master_link_status:up
    master_last_io_seconds_ago:7
    master_sync_in_progress:0
    slave_repl_offset:516
    slave_priority:100
    slave_read_only:1
    connected_slaves:0
    master_replid:22196b425ab42ddf222cc5a64d53acffeb3e63
    master_replid2:0000000000000000000000000000000000000000
    master_repl_offset:516
    second_repl_offset:-1
    repl_backlog_active:1
    repl_backlog_size:1048576
    repl_backlog_first_byte_offset:1
    repl_backlog_histlen:516
    ```


### 编排 redis-sentinel 节点  
也可以把 redis-sentinel 放到和上面的 redis 的同一个 yml 中进行容器编排.此处分成两组,主要是为了两方面:  
- 观察日志方便  
- 确保 redis 主从节点启动之后才启动 redis-sentinel.如果先启动 redis-sentinel 的话,可能触发额外的选举过程,混淆视听.(不是说先启动哨兵不行,而是观察的结果可能存在一定随机性).  


**编写 docker-compose.yml**  
创建 `./yml/redis-sentinel` ,同时 cd 到该目录中.  
注意:每个目录中只能存在一个 docker-compose.yml 文件.  

```yaml
version: '3.7'
services:
  sentinel1:
    image: 'redis:5.0.9'
    container_name: redis-sentinel-1
    restart: always
    command: redis-sentinel /etc/redis/sentinel.conf
    volumes:
      - ./sentinel1.conf:/etc/redis/sentinel.conf
    ports:
      - 26379:26379
  sentinel2:
    image: 'redis:5.0.9'
    container_name: redis-sentinel-2
    restart: always
    command: redis-sentinel /etc/redis/sentinel.conf
    volumes:
      - ./sentinel2.conf:/etc/redis/sentinel.conf
    ports:
      - 26380:26379
  sentinel3:
    image: 'redis:5.0.9'
    container_name: redis-sentinel-3
    restart: always
    command: redis-sentinel /etc/redis/sentinel.conf
    volumes:
      - ./sentinel3.conf:/etc/redis/sentinel.conf
    ports:
      - 26381:26379
networks:
  default:
    external:
      name: redis-data_default
```




**创建配置文件**  
创建 `sentinel1.conf`、`sentinel2.conf`、`sentinel3.conf` ,三份文件的内容是完全相同的,都放到 `/root/redis-sentinel/` 目录中.  

配置内容：  
```conf
bind 0.0.0.0
port 26379
sentinel monitor redis-master redis-master 6379 2
sentinel down-after-milliseconds redis-master 1000
```

**理解 sentinel monitor**  
```
sentinel monitor 主节点名 主节点ip 主节点端口 法定票数
```
- 主节点名,这个是哨兵内部自己起的名字.  
- 主节点 ip,部署 redis-master 的设备 ip.此处由于是使用 docker,可以直接写 docker 的容器名,会被自动 DNS 成对应的容器 ip  
- 主节点端口,不解释.  
- 法定票数,哨兵需要判定主节点是否挂了.但是有的时候可能因为特殊情况,比如主节点仍然工作正常,但是哨兵只有自己网络出问题了,无法访问到主节点了.此时就可能会错误哨兵节点认为主节点下线,出现误判.使用投票的方式来确定主节点是否真的挂了是更稳妥的做法.需要多个哨兵都认为主节点挂了,票数 >= 法定票数之后,才会真的认为主节点是挂了.  


**理解 sentinel down-after-milliseconds**  

```
sentinel down-after-milliseconds 主节点ip 时间
```

主节点和哨兵之间通过心跳包来进行沟通.如果心跳包在指定的时间内还没回来,就视为是节点出现故障.  


>**既然内容相同 为啥要创建多份配置文件?**  
混乱的情况:在运行中可能会对配置进行 rewrite,修改文件内容.如果用一份文件,就可能出现修改的情况.  


**启动所有容器**  
```bash
docker-compose up -d
```
如果启动后发现前面的配置有误,需要重新操作,使用 `docker-compose down` 即可停止并删除刚才创建好的容器.  


 **查看运行日志**  
```bash
docker-compose logs
```
>上述操作必须保证工作目录在 yml 的同级目录中,才能工作.  

可以看到,哨兵节点已经通过主节点,认识到了对应的从节点.  

  
```
# oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
# Redis version=5.0.9, bits=64, commit=00000000, modified=0, pid=1, just started
# Configuration loaded
* Sentinel ID is 4d295260bc4d78e5649a4e1e5c787246baa
* Running mode=sentinel, port=26379.
* Sentinel monitor redis-master redis-master 6379 quorum=2
* +monitor master redis-master 172.22.0.2 6379 # redis-master 172.22.0.2 6379
* +slave slave 172.22.0.3:6379 redis-master 172.22.0.2 6379 # slave 172.22.0.3:6379
* +slave slave 172.22.0.4:6379 redis-master 172.22.0.2 6379 # slave 172.22.0.4:6379
```


 **观察 redis-sentinel 的配置 rewrite**  
再次打开哨兵的配置文件,发现文件内容已经被自动修改了.  

以 `sentinel1.conf` 为例，修改后内容：  
```conf
bind 0.0.0.0
port 26379
sentinel myid 4d295260bc4d78e5649a4e1e5c787246baa
sentinel deny-scripts-reconfig yes
# Generated by CONFIG REWRITE
dir "/data"
sentinel monitor redis-master 172.22.0.2 6379 2
sentinel down-after-milliseconds redis-master 1000
sentinel leader-epoch redis-master 1
sentinel known-replica redis-master 172.22.0.3 6379
sentinel known-replica redis-master 172.22.0.4 6379
sentinel current-epoch 1
# Generated by CONFIG REWRITE
```

对比这三份文件,可以看到配置内容是存在差异的.（因自动生成的 `myid`、`leader-epoch` 等不同 ）



## 重新选举
### redis-master 宕机之后
手动把 redis-master 干掉  
```docker stop redis-master```

观察哨兵的日志
```
# +sdown master redis-master 172.22.0.2 6379
# +odown master redis-master 172.22.0.2 6379 #quorum 3/2
# +new-epoch 1
# +try-failover master redis-master 172.22.0.2 6379
# +vote-for-leader f718caed536d178f5ea6d1316d09407cfae43dd2 1
# 4d2d562860b4cdd478e56494a01e5c787246b6aa voted for 4d2d562860b4cdd478e56494a01e5c787246b6aa 1
# 2ab6de82279bb77f8397c309d36238f51273e80a voted for 4d2d562860b4cdd478e56494a01e5c787246b6aa 1
# +config-update-from sentinel 2ab6de82279bb77f8397c309d36238f51273e80a 172.22.0.5 26379 @ redis-master 172.22.0.2 6379
# +switch-master redis-master 172.22.0.2 6379 172.22.0.4 6379
* +slave slave 172.22.0.3:6379 172.22.0.3 6379 @ redis-master 172.22.0.4 6379
* +slave slave 172.22.0.2:6379 172.22.0.2 6379 @ redis-master 172.22.0.4 6379
# +sdown slave 172.22.0.2:6379 172.22.0.2 6379 @ redis-master 172.22.0.4 6379
```

>这些是Redis Sentinel（哨兵）的日志内容，记录了主节点故障检测、客观下线判定、故障转移（选举新主节点、切换主从关系 ）等过程，比如主节点`redis - master`（IP为`172.22.0.2`，端口`6379` ）先被标记为主观下线（`+sdown` ）、后达成法定票数判定为客观下线（`+odown` ），接着尝试故障转移（`+try - failover` ）、进行领导者投票（`+vote - for - leader` ），最终完成主节点切换（`+switch - master` ），并更新从节点与新主节点（`172.22.0.4 6379` ）的关联等操作 。

可以看到哨兵发现了主节点 down，进一步的由于主节点宕机得票达到 3/2，达到法定得票，于是 master 被判定为 odown。  
- 主观下线 (Subjectively Down, SDown): 哨兵感知到主节点没心跳了.判定为主观下线.  
- 客观下线 (Objectively Down, ODown): 多个哨兵达成一致意见,才能认为 master 确实下线了.  

接下来,哨兵们挑选出了一个新的 master.在上图中,是 172.22.04:6379 这个节点.  
```+switch-master redis-master 172.22.0.2 6379 172.22.0.4 6379```

此时,对于 Redis 来说仍然是可以正常使用的.  


### redis-master 重启之后
手动把 redis-master 启动起来  
```docker start redis-master```

观察哨兵日志  
可以看到刚才新启动的 redis-master 被当成了 slave  
```*convert-to-slave slave 172.22.0.2:6379 172.22.0.2 6379 @ redis-master 172.22.0.4 6379```

使用 redis-cli 也可以进一步的验证这一点  
```
127.0.0.1:6379> info replication
# Replication
role:slave
master_host:172.22.0.4
master_port:6379
master_link_status:up
master_last_io_seconds_ago:0
master_sync_in_progress:0
slave_repl_offset:324475
slave_priority:100
slave_read_only:1
connected_slaves:0
master_replid:ec0cc285a2892fba157318c77ebe1409f9c2254e
master_replid2:0000000000000000000000000000000000000000
master_repl_offset:324475
second_repl_offset:-1
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:318295
repl_backlog_histlen:6181
```


#### 结论
- Redis 主节点如果宕机,哨兵会把其中的一个从节点,提拔成主节点.  
- 当之前的 Redis 主节点重启之后,这个主节点被加入到哨兵的监控中,但是只会被作为从节点使用.  


## 选举原理
假定当前环境如上方介绍,三个哨兵(sentenal1,sentenal2,sentenal3),一个主节点(redis-master),两个从节点(redis-slave1,redis-slave2).  

当主节点出现故障,就会触发重新一系列过程.  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/fda40f8f4af9403c9366d63ea9de258b.png)



### （1) 主观下线  
当 redis-master 宕机,此时 redis-master 和三个哨兵之间的心跳包就没有了.  
此时,站在三个哨兵的角度来看,redis-master 出现严重故障.因此三个哨兵均会把 redis-master 判定为主观下线 (SDown)  


### （2) 客观下线  
此时,哨兵 sentenal1,sentenal2,sentenal3 均会对主节点故障这件事情进行投票.当故障得票数 >= 配置的法定票数之后,  

```sentinel monitor redis-master 172.22.0.4 6379 2```

在这个地方配置的 2,即为法定票数  
此时意味着 redis-master 故障这个事情被做实了.此时触发客观下线 (ODown)  


### （3) 选举出哨兵的 leader  
接下来需要哨兵把剩余的 slave 中挑选出一个新的 master.这个工作不需要所有的哨兵都参与.只需要选出个代表 (称为 leader),由 leader 负责进行 slave 升级到 master 的提拔过程.  

这个选举的过程涉及到 `Raft` 算法  

假定一共三个哨兵节点,S1,S2,S3  
1. 每个哨兵节点都给其他所有哨兵节点,发起一个 "拉票请求".(S1 -> S2, S1 -> S3, S2 -> S1, S2 -> S3, S3 -> S1, S3 -> S2)  
2. 收到拉票请求的节点,会回复一个 "投票响应".响应的结果有两种可能,投 or 不投.  
   >比如 S1 给 S2 发了个投票请求,S2 就会给 S1 返回投票响应.  
   到底 S2 是否要投 S1 呢?取决于 S2 是否给别人投过票了.(每个哨兵只有一票).  
   如果 S2 没有给别人投过票,换而言之,S1 是第一个向 S2 拉票的,那么 S2 就会投 S1.否则则不投.  
3. 一轮投票完成之后,发现得票超过半数的节点,自动成为 leader.  
   >如果出现平票的情况 (S1 投 S2, S2 投 S3, S3 投 S1, 每人一票),就重新再投一次即可.  
   这也是为什么建议哨兵节点设置成奇数个的原因.如果是偶数个,则增大了平票的概率,带来不必要的开销.  
4. leader 节点负责挑选一个 slave 成为新的 master.当其他的 sentenal 发现新的 master 出现了,就说明选举结束了.  

简而言之,Raft 算法的核心就是 "先下手为强".谁率先发出了拉票请求,谁就有更大的概率成为 leader.  
这里的决定因素成了 "网络延时".**网络延时**本身就带有一定随机性.  具体选出的哪个节点是 leader,这个不重要,重要的是能选出一个节点即可.  




### （4) leader 挑选出合适的 slave 成为新的 master  
挑选规则:  
1. 比较**优先级**,优先级高(数值小的)的上位.优先级是配置文件中的配置项 (slave-priority 或者 replica-priority).  
2. 比较 `replication offset` 谁复制的数据多,高的上位.  
3. 比较 `run id` ,谁的 id 小,谁上位.  

（此处有类比示例：蛋哥按优先级、replication offset、run id 标准提拔助教 ）  

当某个 slave 节点被指定为 master 之后,  
1. leader 指定该节点执行 slave no one ,成为 master  
2. leader 指定剩余的 slave 节点,都依附于这个新 master  


## 总结 
上述过程,都是 "无人值守",Redis 自动完成的.这样做就解决了主节点宕机之后需要人工干预的问题,提高了系统的稳定性和可用性.  

一些注意事项:  
- 哨兵节点不能只有一个.否则哨兵节点挂了也会影响系统可用性.  
- 哨兵节点最好是奇数个.方便选举 leader,得票更容易超过半数.  
- 哨兵节点不负责存储数据.仍然是 redis 主从节点负责存储.  
- 哨兵 + 主从复制解决的问题是 "提高可用性",不能解决 "数据极端情况下写丢失" 的问题.  
- 哨兵 + 主从复制不能提高数据的存储容量.当我们需要存的数据接近或者超过机器的物理内存,这样的结构就难以胜任了.  

为了能存储更多的数据,就引入了集群.



